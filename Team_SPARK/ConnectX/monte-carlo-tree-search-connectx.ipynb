{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17592,"databundleVersionId":899221,"sourceType":"competition"}],"dockerImageVersionId":29867,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monte Carlo Tree Search (MCTS)\nThis notebook similates an agent that plays with UCT-MCTS and is based on an implementation written by [Matan Tsipory](https://www.kaggle.com/code/matant/monte-carlo-tree-search-connectx?scriptVersionId=37955585). For an overview of Monte Carlo Tree Search, see the [Wikipedia page](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). Also review the [Python implementation for Tic-Tac-Toe](https://github.com/int8/monte-carlo-tree-search).\n\nA guide for Monte Carlo Tree Search is the [int8.io machine learning blog](https://int8.io/monte-carlo-tree-search-beginners-guide/#Policy_network_training_in_Alpha_Go_and_Alpha_Zero).","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Define ConnectX environment (was defined in v0.1.6)\n!pip install 'kaggle-environments>=0.1.6'\nfrom kaggle_environments import evaluate, make, utils\nfrom functools import lru_cache\nimport numpy as np\nimport random\nimport math\nimport time","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-11-07T05:29:05.151108Z","iopub.execute_input":"2024-11-07T05:29:05.151321Z","iopub.status.idle":"2024-11-07T05:29:13.532228Z","shell.execute_reply.started":"2024-11-07T05:29:05.151294Z","shell.execute_reply":"2024-11-07T05:29:13.531573Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create ConnectX Environment","metadata":{}},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\nconfiguration = env.configuration\nprint(configuration)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"execution":{"iopub.status.busy":"2024-11-07T05:29:13.534182Z","iopub.execute_input":"2024-11-07T05:29:13.534422Z","iopub.status.idle":"2024-11-07T05:29:13.562275Z","shell.execute_reply.started":"2024-11-07T05:29:13.534380Z","shell.execute_reply":"2024-11-07T05:29:13.561492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions for MCTS Agent\nThe implementation below is based on the methods described in [this paper](https://ieeexplore.ieee.org/document/6145622).","metadata":{}},{"cell_type":"code","source":"def MCTS_agent(observation, configuration):\n    \"\"\"\n    Connect X agent based on Monte Carlo Tree Search (MCTS), with optimizations\n    for immediate win/loss detection and cached win-checking.\n    \n    Parameters\n    ----------\n    observation : object\n        The current game observation which includes the board state and player mark.\n    configuration : object\n        Game configuration that contains parameters like the number of rows, columns, \n        in-a-row requirement, and the time limit.\n    \n    Returns\n    -------\n    int\n        The column number where the agent decides to place the piece.\n    \"\"\"\n    import random\n    import math\n    import time\n    global current_state  # so tree can be recycled\n\n    init_time = time.time()\n    EMPTY = 0\n    T_max = configuration.timeout - 0.34  # Time per move, with overhead for safety\n    Cp_base = 1  # Base exploration parameter\n\n    def optimized_play():\n        \"\"\"\n        Makes an optimized play decision using immediate win/loss detection,\n        row tracking, and batched checks for opponent's winning moves.\n\n        Returns\n        -------\n        int\n            The chosen column index for the best move.\n        \"\"\"\n        for col in range(configuration.columns):\n            if row_tracker.lowest_row[col] >= 0:\n                outcome = check_immediate_outcome(board, col, mark, configuration)\n                if outcome == 'win':\n                    return col  # Play winning move\n                elif outcome == 'loss':\n                    continue  # Avoid losing move\n\n        safe_moves = [col for col in range(configuration.columns) if col not in batch_immediate_loss_check(board, mark, configuration)]\n        return safe_moves[0] if safe_moves else 0\n\n    def play(board, column, mark, config):\n        \"\"\"\n        Executes a move on the board by placing the player's mark in the specified column.\n\n        Parameters\n        ----------\n        board : list of int\n            The current board state represented as a 1D list.\n        column : int\n            The column in which to place the mark.\n        mark : int\n            The player's mark (1 or 2).\n        config : object\n            The game configuration with dimensions.\n        \"\"\"\n        columns = config.columns\n        rows = config.rows\n        row = max([r for r in range(rows) if board[column + (r * columns)] == EMPTY])\n        board[column + (row * columns)] = mark\n\n    def is_win(board, column, mark, config):\n        \"\"\"\n        Checks if the last move resulted in a win.\n\n        Parameters\n        ----------\n        board : list of int\n            The current board state.\n        column : int\n            The column where the last mark was placed.\n        mark : int\n            The player's mark (1 or 2).\n        config : object\n            The game configuration including win conditions.\n\n        Returns\n        -------\n        bool\n            True if the move results in a win, False otherwise.\n        \"\"\"\n        columns = config.columns\n        rows = config.rows\n        inarow = config.inarow - 1\n        row = min([r for r in range(rows) if board[column + (r * columns)] == mark])\n\n        def count(offset_row, offset_col):\n            run = 0\n            for i in range(1, inarow + 1):\n                r, c = row + offset_row * i, column + offset_col * i\n                if r < 0 or r >= rows or c < 0 or c >= columns or board[c + r * columns] != mark:\n                    break\n                run += 1\n            return run\n    \n        return (\n                count(1, 0) >= inarow  # vertical.\n                or (count(0, 1) + count(0, -1)) >= inarow  # horizontal.\n                or (count(-1, -1) + count(1, 1)) >= inarow  # top left diagonal.\n                or (count(-1, 1) + count(1, -1)) >= inarow  # top right diagonal.\n        )\n\n    @lru_cache(maxsize=10000)\n    def is_win_cached(board, column, mark, config):\n        \"\"\"\n        Checks if placing a piece in the specified column leads to a win for the specified player,\n        using a cached version of the is_win function to improve efficiency.\n    \n        Parameters\n        ----------\n        board : np.ndarray\n            The current board state.\n        column : int\n            Column index where the piece was placed.\n        mark : int\n            The player's mark (1 or 2).\n        config : Config\n            Game configuration, including the win length and board dimensions.\n    \n        Returns\n        -------\n        bool\n            True if placing the piece results in a win for the player, otherwise False.\n        \"\"\"\n        return is_win(board, column, mark, config)\n\n    def play_and_check_win(board, column, mark, config):\n        \"\"\"\n        Plays a piece in the specified column and checks if it results in a win.\n    \n        Parameters\n        ----------\n        board : np.ndarray\n            The current board state.\n        column : int\n            Column index where the piece is to be placed.\n        mark : int\n            The player's mark (1 or 2).\n        config : Config\n            Game configuration, including the win length and board dimensions.\n    \n        Returns\n        -------\n        bool\n            True if placing the piece results in a win for the player, otherwise False.\n        \"\"\"\n        temp_board = board.copy()\n        play(temp_board, column, mark, config)\n        return is_win_cached(temp_board, column, mark, config)\n\n    def check_immediate_outcome(board, column, mark, config):\n        \"\"\"\n        Determines if playing in the specified column leads to an immediate win\n        for the current player or loss (win for opponent).\n    \n        Parameters\n        ----------\n        board : np.ndarray\n            The current board state.\n        column : int\n            Column index where the piece is to be placed.\n        mark : int\n            The player's mark (1 or 2).\n        config : Config\n            Game configuration, including the win length and board dimensions.\n    \n        Returns\n        -------\n        str or None\n            'win' if the move results in a win for the player, 'loss' if the move leads to an\n            immediate win for the opponent, and None if neither is true.\n        \"\"\"\n        if play_and_check_win(board, column, mark, config):\n            return 'win'\n        \n        opponent = opponent_mark(mark)\n        if play_and_check_win(board, column, opponent, config):\n            return 'loss'\n        \n        return None\n\n    def batch_immediate_loss_check(board, mark, config):\n        \"\"\"\n        Checks all columns to see if any of them leads to an immediate loss (win for opponent)\n        in one function call, reducing redundant play-check cycles.\n    \n        Parameters\n        ----------\n        board : np.ndarray\n            The current board state.\n        mark : int\n            The player's mark (1 or 2).\n        config : Config\n            Game configuration, including the win length and board dimensions.\n    \n        Returns\n        -------\n        list of int\n            List of columns that would result in an immediate loss if played.\n        \"\"\"\n        opponent = opponent_mark(mark)\n        losing_moves = []\n        for column in range(config.columns):\n            if board[0][column] == 0:  # Only consider non-full columns\n                temp_board = board.copy()\n                play(temp_board, column, opponent, config)\n                if is_win_cached(temp_board, column, opponent, config):\n                    losing_moves.append(column)\n        return losing_moves\n\n    def is_tie(board):\n        \"\"\"\n        Checks if the board is full, resulting in a tie.\n\n        Parameters\n        ----------\n        board : list of int\n            The current board state.\n\n        Returns\n        -------\n        bool\n            True if the board is full, indicating a tie.\n        \"\"\"\n        return not(any(mark == EMPTY for mark in board))\n\n    def check_finish_and_score(board, column, mark, config):\n        \"\"\"\n        Evaluates if the game has finished and provides a score.\n\n        Parameters\n        ----------\n        board : list of int\n            The current board state.\n        column : int\n            The column of the last move.\n        mark : int\n            The player's mark.\n        config : object\n            The game configuration.\n\n        Returns\n        -------\n        tuple\n            (bool, float): First element is True if the game is finished; the second is the score.\n        \"\"\"\n        if is_win(board, column, mark, config):\n            return (True, 1)\n        if is_tie(board):\n            return (True, 0.5)\n        else:\n            return (False, None)\n\n    def uct_score(node_total_score, node_total_visits, parent_total_visits, Cp=Cp_base):\n        \"\"\"\n        Calculates the UCB1 score for balancing exploration and exploitation.\n\n        Parameters\n        ----------\n        node_total_score : float\n            Total score accumulated by the node.\n        node_total_visits : int\n            Number of visits to the node.\n        parent_total_visits : int\n            Number of visits to the parent node.\n        Cp : float, optional\n            Exploration constant, by default Cp_base.\n\n        Returns\n        -------\n        float\n            The UCB1 score.\n        \"\"\"\n        if node_total_visits == 0:\n            return math.inf\n        return node_total_score / node_total_visits + Cp * math.sqrt(\n            2 * math.log(parent_total_visits) / node_total_visits)\n\n    def opponent_mark(mark):\n        \"\"\"\n        Determines the opponent's mark.\n    \n        Parameters\n        ----------\n        mark : int\n            The mark of the current player (1 or 2).\n    \n        Returns\n        -------\n        int\n            The mark of the opponent player (2 if the input mark is 1, and 1 if the input mark is 2).\n        \"\"\"\n        return 3 - mark\n\n    def opponent_score(score):\n        \"\"\"\n        Returns the opponent's score, used in score backpropagation in the MCTS process.\n    \n        Parameters\n        ----------\n        score : float\n            The score of the current player in the MCTS simulation (ranging from 0.0 to 1.0).\n    \n        Returns\n        -------\n        float\n            The score from the opponent's perspective, calculated as `1 - score`.\n        \"\"\"\n        return 1 - score\n\n    def random_action(board, config):\n        \"\"\"\n        Selects a random legal move from the available columns, avoiding moves that would \n        immediately lose the game.\n    \n        Parameters\n        ----------\n        board : list of int\n            The current board state represented as a 1D list.\n        config : object\n            The game configuration containing parameters such as the number of columns and rows.\n    \n        Returns\n        -------\n        int\n            The column index of a randomly selected legal move.\n        \"\"\"\n        legal_moves = [c for c in range(config.columns) if board[c] == EMPTY]\n        safe_moves = [c for c in legal_moves if not is_immediate_loss(board, c, config)]\n        return random.choice(safe_moves) if safe_moves else random.choice(legal_moves)\n\n    def is_immediate_loss(board, column, config):\n        \"\"\"\n        Check if placing a piece in the specified column results in an immediate loss for the player.\n    \n        This function temporarily plays a piece for the opponent in the given column and checks if it\n        creates a winning condition for the opponent. It is useful to filter out moves that would \n        allow the opponent to win immediately in their next turn.\n    \n        Parameters\n        ----------\n        board : list of int\n            The current state of the game board represented as a 1D list.\n            Each element corresponds to a slot, where an integer indicates the player mark \n            or an empty slot.\n        column : int\n            The column index where the piece is considered to be placed.\n        config : object\n            The game configuration object containing board dimensions and win conditions. \n            Attributes include:\n                - `columns` (int): The number of columns on the board.\n                - `rows` (int): The number of rows on the board.\n                - `inarow` (int): The number of consecutive pieces required to win.\n\n        Returns\n        -------\n        bool\n            True if placing a piece in the specified column results in an immediate loss,\n            meaning the opponent can win in the next turn; False otherwise.\n    \n        Notes\n        -----\n        This function creates a temporary copy of the board and does not modify the original board state.\n        It assumes that `play` and `is_win` functions are available to simulate moves and check for \n        winning conditions, respectively.\n        \"\"\"\n        temp_board = board.copy()\n        play(temp_board, column, opponent_mark(observation.mark), config)\n        return is_win(temp_board, column, opponent_mark(observation.mark), config)\n\n    def default_policy_simulation(board, mark, config):\n        \"\"\"\n        Simulates a game using random moves until completion, returning the score.\n\n        Parameters\n        ----------\n        board : list of int\n            Current board state.\n        mark : int\n            Starting player's mark.\n        config : object\n            Game configuration.\n\n        Returns\n        -------\n        float\n            Resulting game score for the starting player.\n        \"\"\"\n        original_mark = mark\n        board = board.copy()\n        column = random_action(board, config)\n        play(board, column, mark, config)\n        is_finish, score = check_finish_and_score(board, column, mark, config)\n        while not is_finish:\n            mark = opponent_mark(mark)\n            column = random_action(board, config)\n            play(board, column, mark, config)\n            is_finish, score = check_finish_and_score(board, column, mark, config)\n        if mark == original_mark:\n            return score\n        return opponent_score(score)\n    \n    def find_action_taken_by_opponent(new_board, old_board, config):\n        \"\"\"\n        Identifies the column where the opponent has made a move, based on the difference\n        between the new and old board states. This function is used for reusing the MCTS tree \n        between turns.\n    \n        Parameters\n        ----------\n        new_board : list of int\n            The updated board state after the opponent's move.\n        old_board : list of int\n            The board state before the opponent's move.\n        config : object\n            Game configuration, which includes properties like the number of columns.\n    \n        Returns\n        -------\n        int\n            The column index where the opponent placed their piece.\n            Returns -1 if no difference is found, which should not occur in normal gameplay.\n        \"\"\"\n        for i, piece in enumerate(new_board):\n            if piece != old_board[i]:\n                return i % config.columns\n        return -1  # shouldn't get here\n\n    def dynamic_cp_adjustment(children_scores, base_cp=Cp_base, max_increase=3, min_cp=0.1):\n        \"\"\"\n        Adjusts Cp dynamically based on the variance in children scores.\n    \n        Parameters\n        ----------\n        children_scores : list of float\n            List of scores from child nodes of the current state.\n        base_cp : float, optional\n            Base Cp for exploration, by default Cp_base.\n        max_increase : float, optional\n            Maximum factor by which to increase Cp, by default 3.\n        min_cp : float, optional\n            Minimum Cp value to maintain exploration, by default 0.1.\n    \n        Returns\n        -------\n        float\n            Adjusted Cp based on variance in child scores, capped at max_increase and min_cp.\n        \"\"\"\n        if len(children_scores) < 2:  # Avoid calculating variance without at least 2 scores\n            return base_cp\n    \n        # Calculate the mean of the children_scores\n        mean_score = sum(children_scores) / len(children_scores)\n        \n        # Calculate the standard deviation\n        variance = sum((x - mean_score) ** 2 for x in children_scores) / len(children_scores)\n        std_dev = variance ** 0.5\n\n        # Adjust Cp based on the standard deviation\n        adjusted_cp = base_cp * (1 + std_dev)\n        adjusted_cp = max(adjusted_cp, min_cp)  # Upper bound of cp\n        adjusted_cp = min(adjusted_cp, base_cp * max_increase)  # Lower bound of cp\n    \n        return adjusted_cp\n\n    class State():\n        \"\"\"\n        Represents a node in the game tree for Monte Carlo Tree Search (MCTS).\n\n        Attributes\n        ----------\n        board : list of int\n            The current board state represented as a 1D list.\n        mark : int\n            The current player's mark (1 or 2).\n        config : object\n            Game configuration, containing parameters like rows, columns, and in-a-row requirement.\n        parent : State, optional\n            The parent node in the tree. None if this is the root.\n        children : list of State\n            List of child nodes representing possible moves from the current state.\n        node_total_score : float\n            The cumulative score of this node from all simulations.\n        node_total_visits : int\n            The number of times this node has been visited during MCTS.\n        is_terminal : bool\n            Indicates whether this node represents a terminal state (win or tie).\n        terminal_score : float, optional\n            Score of the terminal state if `is_terminal` is True.\n        action_taken : int, optional\n            The column index of the move taken to reach this state from the parent.\n        available_moves : list of int\n            List of columns where a move can be legally played from this state.\n        expandable_moves : list of int\n            Subset of `available_moves` that have not yet been explored.\n        \"\"\"\n        def __init__(self, board, mark, config, parent=None, is_terminal=False, terminal_score=None, action_taken=None):\n            \"\"\"\n            Initializes a State instance.\n    \n            Parameters\n            ----------\n            board : list of int\n                The current board state.\n            mark : int\n                The player's mark (1 or 2).\n            config : object\n                Game configuration, containing parameters like rows, columns, and in-a-row requirement.\n            parent : State, optional\n                The parent state in the MCTS tree, by default None.\n            is_terminal : bool, optional\n                Whether this state is a terminal node, by default False.\n            terminal_score : float, optional\n                The score of the terminal state, if applicable, by default None.\n            action_taken : int, optional\n                The column index of the action taken to reach this state, by default None.\n            \"\"\"\n            self.board = board.copy()\n            self.mark = mark\n            self.config = config\n            self.children = []\n            self.parent = parent\n            self.node_total_score = 0\n            self.node_total_visits = 0\n            self.available_moves = [c for c in range(config.columns) if board[c] == EMPTY]\n            self.expandable_moves = self.available_moves.copy()\n            self.is_terminal = is_terminal\n            self.terminal_score = terminal_score\n            self.action_taken = action_taken\n\n        def is_expandable(self):\n            \"\"\"\n            Checks if the current node has unexplored child nodes, which indicates\n            the node can still be expanded by exploring unvisited moves.\n    \n            Returns\n            -------\n            bool\n                True if there are still moves to explore from this state; False otherwise.\n            \"\"\"\n            return (not self.is_terminal) and (len(self.expandable_moves) > 0)\n\n        def expand_and_simulate_child(self):\n            \"\"\"\n            Expands a new child node by selecting a random move from `expandable_moves`,\n            then simulates the game outcome from this child node and backpropagates the score.\n    \n            This method performs the Expansion, Simulation, and Backpropagation steps of MCTS.\n            \"\"\"\n            column = random.choice(self.expandable_moves)\n            child_board = self.board.copy()\n            play(child_board, column, self.mark, self.config)\n            is_terminal, terminal_score = check_finish_and_score(child_board, column, self.mark, self.config)\n            self.children.append(State(child_board, opponent_mark(self.mark),\n                                       self.config, parent=self,\n                                       is_terminal=is_terminal,\n                                       terminal_score=terminal_score,\n                                       action_taken=column\n                                       ))\n            simulation_score = self.children[-1].simulate()\n            self.children[-1].backpropagate(simulation_score)\n            self.expandable_moves.remove(column)\n\n        def choose_strongest_child(self, base_cp):\n            \"\"\"\n            Selects the child node with the highest Upper Confidence Bound (UCB1) score \n            using dynamically adjusted Cp.\n        \n            Parameters\n            ----------\n            base_cp : float\n                The base exploration parameter for UCB1.\n        \n            Returns\n            -------\n            State\n                The child node with the highest UCB1 score.\n            \"\"\"\n            # Calculate initial UCB1 scores for each child with the given Cp\n            children_scores = [uct_score(child.node_total_score,\n                                          child.node_total_visits,\n                                          self.node_total_visits,\n                                          base_cp) for child in self.children]\n            \n            # Dynamically adjust Cp based on the calculated scores\n            dynamic_cp = dynamic_cp_adjustment(children_scores, base_cp)\n            \n            # Recalculate UCB1 scores with the dynamically adjusted Cp\n            adjusted_scores = [uct_score(child.node_total_score,\n                                          child.node_total_visits,\n                                          self.node_total_visits,\n                                          dynamic_cp) for child in self.children]\n            \n            # Find the index of the child with the maximum adjusted score\n            best_child_index = adjusted_scores.index(max(adjusted_scores))\n            \n            return self.children[best_child_index]\n            \n        def choose_play_child(self):\n            \"\"\"\n            Selects the child node with the highest cumulative score as the \n            next move choice for the agent.\n    \n            Returns\n            -------\n            State\n                The child node with the highest cumulative score.\n            \"\"\"\n            children_scores = [child.node_total_score for child in self.children]\n            max_score = max(children_scores)\n            best_child_index = children_scores.index(max_score)\n            return self.children[best_child_index]\n\n        def tree_single_run(self):\n            \"\"\"\n            Executes a single iteration of the MCTS process, encompassing the \n            selection, expansion, simulation, and backpropagation phases.\n    \n            If the current node is terminal, it only backpropagates its score.\n            Otherwise, it either expands a child node if possible or continues \n            to traverse the tree by selecting the strongest child.\n    \n            Returns\n            -------\n            None\n            \"\"\"\n            if self.is_terminal:\n                self.backpropagate(self.terminal_score)\n                return\n            if self.is_expandable():\n                self.expand_and_simulate_child()\n                return\n            self.choose_strongest_child(Cp_base).tree_single_run()\n\n        def simulate(self):\n            \"\"\"\n            Runs a simulation (playout) from the current node to a terminal state using random moves.\n            \n            If the node is terminal, returns its score; otherwise, calls the default policy simulation.\n    \n            Returns\n            -------\n            float\n                The resulting score from the simulation for the current player.\n            \"\"\"\n            if self.is_terminal:\n                return self.terminal_score\n            return opponent_score(default_policy_simulation(self.board, self.mark, self.config))\n\n        def backpropagate(self, simulation_score):\n            \"\"\"\n            Updates this node's score and visit count, then propagates the updated score to its ancestors.\n    \n            Parameters\n            ----------\n            score : float\n                The score resulting from a simulation to be backpropagated.\n            \"\"\"\n            self.node_total_score += simulation_score\n            self.node_total_visits += 1\n            if self.parent is not None:\n                self.parent.backpropagate(opponent_score(simulation_score))\n                \n        def choose_child_via_action(self, action):\n            \"\"\"\n            Retrieves the child node that corresponds to a specific action taken \n            from the current state, allowing tree reuse between agent moves.\n    \n            Parameters\n            ----------\n            action : int\n                The column index of the move that was taken to transition to this child.\n    \n            Returns\n            -------\n            State or None\n                The child node representing the given action, or None if no child matches.\n            \"\"\"\n            for child in self.children:\n                if child.action_taken == action:\n                    return child\n            return None\n\n    class RowTracker:\n        \"\"\"\n        Tracks the lowest available row in each column of the board, allowing quick placement\n        of pieces without iterating over each row.\n    \n        Attributes\n        ----------\n        lowest_row : list of int\n            Tracks the lowest empty row for each column.\n        config : Config\n            Game configuration, including the win length and board dimensions.\n        \"\"\"\n    \n        def __init__(self, board, config):\n            \"\"\"\n            Initializes RowTracker with the current board state and configuration.\n    \n            Parameters\n            ----------\n            board : np.ndarray\n                The current board state.\n            config : Config\n                Game configuration, including the win length and board dimensions.\n            \"\"\"\n            self.config = config\n            self.lowest_row = [self.find_lowest_row(board, col) for col in range(config.columns)]\n    \n        def find_lowest_row(self, board, column):\n            \"\"\"\n            Finds the lowest empty row in a specified column.\n    \n            Parameters\n            ----------\n            board : np.ndarray\n                The current board state.\n            column : int\n                The column index.\n    \n            Returns\n            -------\n            int\n                The lowest empty row index, or -1 if the column is full.\n            \"\"\"\n            for row in range(self.config.rows - 1, -1, -1):\n                if board[row][column] == 0:\n                    return row\n            return -1\n    \n        def update_after_play(self, column):\n            \"\"\"\n            Updates the lowest row tracker after a piece is played in the specified column.\n    \n            Parameters\n            ----------\n            column : int\n                Column index where the piece was placed.\n            \"\"\"\n            self.lowest_row[column] -= 1\n\n    board = observation.board\n    mark = observation.mark\n    \n    # If current_state already exists, recycle it based on action taken by opponent\n    try:  \n        current_state = current_state.choose_child_via_action(\n            find_action_taken_by_opponent(board, current_state.board, configuration))\n        current_state.parent = None  # make current_state the root node, dereference parents and siblings\n        \n    except:  # new game or other error in recycling attempt due to Kaggle mechanism\n        current_state = State(board, mark,  # This state is considered after the opponent's move\n                              configuration, parent=None, is_terminal=False, terminal_score=None, action_taken=None)\n   \n    # Run MCTS iterations until time limit is reached.\n    while time.time() - init_time <= T_max:\n        current_state.tree_single_run()\n        \n    current_state = current_state.choose_play_child()\n    return current_state.action_taken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T05:29:13.564254Z","iopub.execute_input":"2024-11-07T05:29:13.564581Z","iopub.status.idle":"2024-11-07T05:29:13.654235Z","shell.execute_reply.started":"2024-11-07T05:29:13.564544Z","shell.execute_reply":"2024-11-07T05:29:13.653627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check agent validity","metadata":{}},{"cell_type":"code","source":"def reset_env():\n    try:\n        del current_state  # Only delete if current_state has been assigned\n    except NameError:\n        pass  # Ignore if current_state is not defined\n\n# Reset the environment and clear any existing tree state\nreset_env()\n\n# Run the environment\nenv.run([MCTS_agent, MCTS_agent])\n\n# Check if the agents have completed successfully\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T05:29:13.655523Z","iopub.execute_input":"2024-11-07T05:29:13.655827Z","iopub.status.idle":"2024-11-07T05:31:14.905409Z","shell.execute_reply.started":"2024-11-07T05:29:13.655787Z","shell.execute_reply":"2024-11-07T05:31:14.904670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate your agent","metadata":{}},{"cell_type":"code","source":"def mean_reward(rewards):\n    \"\"\"\n    Calculates the mean reward for a set of rewards, weighted by the number of trials.\n\n    Parameters\n    ----------\n    rewards : list of tuple of int\n        A list of tuples where each tuple contains two integers:\n        - The first integer represents the total number of successful trials (rewards).\n        - The second integer represents the total number of unsuccessful trials.\n\n    Returns\n    -------\n    float\n        The mean reward, calculated as the sum of successful trials divided by\n        the sum of all trials (both successful and unsuccessful).\n    \n    Notes\n    -----\n    This function computes a weighted mean, where each reward tuple's weight is\n    determined by the total number of trials in that tuple.\n    \n    Example\n    -------\n    >>> mean_reward([(3, 2), (4, 1), (2, 3)])\n    0.6\n    \"\"\"\n    # Calculate the total number of successful trials across all rewards\n    total_rewards = sum(r[0] for r in rewards)\n    \n    # Calculate the total number of trials (successful + unsuccessful)\n    total_trials = sum(r[0] + r[1] for r in rewards)\n    \n    # Return the weighted mean of successful trials\n    return total_rewards / total_trials\n    \n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [MCTS_agent, \"random\"], num_episodes=20)))\nprint(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", MCTS_agent], num_episodes=20)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [MCTS_agent, \"negamax\"], num_episodes=5)))\nprint(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", MCTS_agent], num_episodes=5)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T05:31:14.908092Z","iopub.execute_input":"2024-11-07T05:31:14.908319Z","iopub.status.idle":"2024-11-07T06:01:44.409976Z","shell.execute_reply.started":"2024-11-07T05:31:14.908292Z","shell.execute_reply":"2024-11-07T06:01:44.409250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> # Play your Agent\nClick on any column to place a checker there (\"manually select action\").","metadata":{}},{"cell_type":"code","source":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([MCTS_agent, None], width=500, height=450)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T06:01:44.411446Z","iopub.execute_input":"2024-11-07T06:01:44.411717Z","iopub.status.idle":"2024-11-07T06:01:49.101817Z","shell.execute_reply.started":"2024-11-07T06:01:44.411688Z","shell.execute_reply":"2024-11-07T06:01:49.101141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Write Submission File\n\n","metadata":{}},{"cell_type":"code","source":"import inspect\n\nsubmission_path = \"submission.py\"\n        \ndef write_agent_to_file(function, file):\n    with open(file, \"w\") as f:\n        f.write(\"from functools import lru_cache\\n\")\n        f.write(\"import numpy as np\\n\")\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(MCTS_agent, submission_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T06:01:49.103019Z","iopub.execute_input":"2024-11-07T06:01:49.103265Z","iopub.status.idle":"2024-11-07T06:01:49.134186Z","shell.execute_reply.started":"2024-11-07T06:01:49.103237Z","shell.execute_reply":"2024-11-07T06:01:49.133243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","metadata":{}},{"cell_type":"code","source":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\ntry:\n    submission = utils.read_file(\"/kaggle/working/submission.py\")\n    agent = utils.get_last_callable(submission)\nfinally:\n    sys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T06:01:49.135462Z","iopub.execute_input":"2024-11-07T06:01:49.135767Z","iopub.status.idle":"2024-11-07T06:04:37.017297Z","shell.execute_reply.started":"2024-11-07T06:01:49.135724Z","shell.execute_reply":"2024-11-07T06:04:37.016471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}